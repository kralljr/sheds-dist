---
title: "Simulation study exploring exposure distribution changes"
output:
  html_document:
    toc: true
    number_sections: true
    theme: cosmo
    toc_depth: 2
---
		  
```{r lib, warning = F, message = F, echo = F}
library(dplyr)
library(reshape2)
library(knitr)
library(ggplot2)
library(RColorBrewer)
library(fda)
library(fda.usc)
library(truncnorm)
library(gridExtra)
library(MESS)
library(refund)
library(glmnet)


fp1 <- "~/Documents/SHEDS"
fp2 <- "~/Dropbox/SHEDS/data"
opts_chunk$set(message = F, fig.align = "center", warning = F, echo = F)

eval1 <- T
figh1 <- 4
figw1 <- 8
```



```{r external}
source("sim_study_fn.R")
source("sim_study_display.R")
source("mcmc_corr.R")
```

Jenna Krall

Revised 9/9/15

The current project is aimed at using distributions of exposure to outdoor pollution as predictors in a health effects regression model.  While previous studies only had access to daily summaries of exposure (e.g. daily mean), recently simulated exposure data has become available and provides a distribution of exposures for each day.  Having exposure distributions for each day will allow us to determine the changes in exposure distribution that are associated with the greatest increases in adverse health outcomes.  Additionally, we can determine the associations between quantiles of the exposure distribution and adverse health outcomes.  This current analysis is focused on the question of how quantiles of the exposure distribution are associated with adverse health outcomes.  Knowing whether some quantiles are more associated with adverse health outcomes can help to develop more targeted public health campaigns.  For example, if the 20\% quantile is most associated with adverse health outcomes, then those individuals who generally have lower exposures to outdoor pollution should be targeted.




```{r argv}
na1 <- 30
argvals2 <- seq(0.05, 0.95, length = na1)


disttype1 <- "norm"
sd2 <- 0.001
```

# Data

The Atlanta SHEDS data were scaled up from ZCTAs by sampling from each ZCTA proportional to its population (with the largest ZCTA, 30044, sampling all 100 individuals).  There were 193 ZCTAs in the file.  By sampling up to Atlanta, we obtained 1461 days from 1999-2002.   


# Simulation study

## Methods 

To conduct the simulation study, we utilized SHEDS exposures and simulated ED visits for four different assumed associations between exposure distributions and daily ED visits.   







```{r wtrue, fig.height = figh1, fig.width = figw1}

#Detrend date?

# Get all data
xall <- read.csv(file.path(fp2, "SHEDS_pm_atl.csv"))[, c(2, 4)]
xall[, 1] <- as.Date(xall[, 1])

# Get quantiles
quants1 <- tapply(xall$pm, xall$date, quantile, probs = argvals2)
quants2 <- sapply(quants1, function(x) x)

# Get quantiles
quantsREG <- tapply(xall$pm, xall$date, quantile, probs = ag1)
quantsREG <- t(sapply(quantsREG, function(x) x))





# Get in form for ggplot
x1 <- quants2
dates <- unique(xall[, 1])
samp1 <- sample(dates, 100)

# Make smaller data
mxall <- xall[which(xall[, 1] %in% samp1),]


samp1 <- samp1[1]
samp2 <- paste0("x", gsub("-", ".", samp1))


# Get in form for ggplot
d1 <- data.frame(argvals2, x1)
mx <- melt(d1, id.vars = "argvals2")
mx$variable <- tolower(mx$variable)

# Select one day to highlight
mx <- mutate(mx, select1 = ifelse(variable == samp2, "Yes", "No"))
mxall <- mutate(mxall, select1 = ifelse(date == samp1, "Yes", "No"))

# Order so plot highlighted last
lev1 <- mxall$date[order(mxall$select1)]
mxall$date <- factor(mxall$date, levels = unique(lev1))

lev1 <- mx$variable[order(mx$select1)]
mx$variable <- factor(mx$variable, levels = unique(lev1))

# Create density plots of all x's
g1 <- ggplot(data = mxall, aes(x = pm, group = date, colour = select1)) +
  geom_density(aes(size = select1)) + theme_bw() +
  scale_colour_manual(guide = F, values = c("grey80", "red")) +
  scale_size_manual(guide = F, values = c(1, 1.4)) +
  theme(legend.position = "none") +
  xlab("Concentration") + ggtitle("Density plots of personal exposure")

# Create quantile plots of x quantiles
g2 <- ggplot(data = mx, aes(x = argvals2, y = value, group = variable, colour = select1)) +
  geom_line(aes(size = select1)) + theme_bw() +

  scale_colour_manual(guide = F, values = c("grey80", "red")) +
  scale_size_manual(guide = F, values = c(1, 1.4)) +

  theme(legend.position = "none") + xlab("Quantile") +
  ylab("Concentration") + ggtitle("Plot of quantiles of concentration")

# Arrange output for plots 
grid.arrange(g1, g2, ncol = 2)




# Save output
x1 <- list()
x1$xall <- t(xall)

xM <- quantsREG[, "50%"]

x1$x1 <- quants2
x1$xREG <- t(quantsREG)


x1$xfn <- getxfn(x1$x1, argvals2, ns1 = ns2) 
x1$basis1 <- x1$xfn$basis1
x1$xfn <- x1$xfn$xfn
x1$xM <- xM

xtrue <- x1

```

## Simulated associations


We consider 6 functions for beta(q) for a constant value $v$:

1. Constant beta, $\beta$(q) = $v$
2. Larger beta for low and high quantiles, $\beta$(q) = $v$ + 1/4 * (q - 0.5)^2
3. Larger beta for low quantiles,  $\beta$(q) = $v$ + 1/10 * exp(q * -7)
4. Larger beta for high quantiles, $\beta$(q) = $v$ + 1 / 10000 * exp(q * 7)  

These beta functions were chosen to reflect the scenarios where the association doesn't vary by quantile (constant beta), the lower and upper quantiles have a greater effect (if these represent more susceptible individuals), the low quantiles have a greater effect, and the high quantiles have a greater effect.  These beta functions are shown below in the results as the black curves.


```{r totb}
shift1 <- 0.01
scaleb <- 1/2

auc1 <- vector()
typeb <- c("constant", "x2", "high", "low")
for(i in 1 : 4) {
  betaf <- getbeta(typeb[i], val = shift1, scale = scaleb)
  bf <- betaf(argvals2)
  auc1[i] <- auc(argvals2, bf)
  print(100 * (exp(auc1[i] * 10) - 1))
}

```

# Generating outcome data

Once we chose a distribution of exposures and a beta function, we generated outcomes y as Poisson($\mu$) where $\mu=\exp\{\beta_0 + \int_0^1 \beta(q) X(q) dq\}$ or Normal($\mu$, 0.01), where $\mu=\beta_0 + \int_0^1 \beta(q) X(q)$.  The integral is approximated by $\frac{1}{N}\sum_{i=1}^N \beta_i X_i$ where $N$ is the number of quantiles included and $X_i$ is the concentration at quantile $i$.  

<!--To approximate the integral, we multiplied the observed x values by the beta function at the observed quantiles and computed the area under the curve.
-->

# Simulation study

## Methods

We generated quantile data from our exposures for 20 quantiles from 0 to 1.  For each simulated scenario (choice of distributions for exposures, and choice of $\beta(q)$), we simulated outcomes $Y$ using poisson or gaussian distributions.   


To compare our results with more traditional approaches, we fit the quantiles using both univariate regression (green) and multivariate regression (orange), using linear or log-linear regression models, depending on how $y$ was simulated.  We assumed that the researcher selected those quantiles of primary interest: 10\%, 25\%, 50\%, 75\%, 90\%.   

## Results









```{r getx, eval = T, fig.height = 10, fig.width = 15}


ny <- 1000

  
# for each reasonable comb
ts1 <- c("constant", "high", "low", "high", "low", "x2", "high", "low", "x2")
xs1 <- c("shift", "longr", "longl", "wide", "wide", "wide", "true", "true", "true")
xs1 <- data.frame(xs1, c(1, 2, 3, 4, 4, 4, 5, 5, 5))


dt1 <- "norm"


tx <- unique(xs1[, 1])

x1use <- list()
# For each x
for(xi in 1 : 5) {

  if(xi < 5) {
    x1use[[xi]] <- getx(ny, naG, argvals2, argvalslr = ag1, typex = tx[xi], ns1 = ns2)
  } else if (xi == 5){  
    x1use[[xi]] <- xtrue
}}




```




```{r setall}
cn <- c("X", "Est", "SE", "Zval", "Pval", "Type1", "Reg")
std1 <- T
lb1 <- -0.1
ub1 <- 0.25
```






### No shift




#### Small betas
```{r beta1, eval = T, fig.height = 10, fig.width = 15}


scaleb <- 1/2
shift1 <- 0

runsim1 <- runsim(x1use, xs1, ts1, cn, scaleb = scaleb, val1 = shift1, disttype1 = dt1, lb1 = lb1, ub1 = ub1, argvalslr = )                    
gfun(runsim1, lb1 = lb1, ub1 = ub1)

save(runsim1, file = "sbeta_noshift.RData")

tab1 <- data.frame(levels(runsim1$xfull$Type1), round(runsim1$med, 3))
colnames(tab1) <- c("Type", "Beta_median")
kable(tab1)
```


#### Larger betas
```{r beta2, eval = T, fig.height = 10, fig.width = 15}
scaleb <- 1

runsim1 <- runsim(x1use, xs1, ts1, cn, scaleb = scaleb, val1 = shift1, disttype1 = dt1, lb1 = lb1, ub1 = ub1)                    
gfun(runsim1, lb1 = lb1, ub1 = ub1)


save(runsim1, file = "lbeta_noshift.RData")

tab1 <- data.frame(levels(runsim1$xfull$Type1), round(runsim1$med, 3))
colnames(tab1) <- c("Type", "Beta_median")
kable(tab1)
```





### Shift

#### Small betas
```{r beta3, eval = T, fig.height = 10, fig.width = 15}


scaleb <- 1/2
shift1 <- 0.01

runsim1 <- runsim(x1use, xs1, ts1, cn, scaleb = scaleb, val1 = shift1, disttype1 = dt1, lb1 = lb1, ub1 = ub1)                    
gfun(runsim1, lb1 = lb1, ub1 = ub1)


save(runsim1, file = "sbeta_shift.RData")


tab1 <- data.frame(levels(runsim1$xfull$Type1), round(runsim1$med, 3))
colnames(tab1) <- c("Type", "Beta_median")
kable(tab1)
```


#### Larger betas
```{r beta4, eval = T, fig.height = 10, fig.width = 15}
scaleb <- 1

runsim1 <- runsim(x1use, xs1, ts1, cn, scaleb = scaleb, val1 = shift1, disttype1 = dt1, lb1 = lb1, ub1 = ub1)                    
gfun(runsim1, lb1 = lb1, ub1 = ub1)


save(runsim1, file = "lbeta_shift.RData")

tab1 <- data.frame(levels(runsim1$xfull$Type1), round(runsim1$med, 3))
colnames(tab1) <- c("Type", "Beta_median")
kable(tab1)

```





```{r, eval = F}
#8/31/15
# stdize, don't scale

# try by hand
bf <- getbeta("high")


ag1 <- c(0.1, 0.25, 0.5, 0.75, 0.9)
ag1 <- argvals2

betas1 <- bf(ag1)  

xuse1 <- x1use[[4]]$xall  
xuse <- t(apply(xuse1,  2, quantile, probs = ag1))


linfun <-  rowSums(sweep(xuse, 2, betas1, "*"))
set.seed(10)
y <- rpois(ny, exp(linfun))


#stdize
mn1 <- apply(xuse, 2, mean)
sd1 <- apply(xuse, 2, sd)
xuse <- sweep(xuse, 2, mn1, "-")
xuse <- sweep(xuse, 2, sd1, "/")




#check reality
meds <- apply(xuse1, 2, median)
summary(linfun / meds)

glm1 <- glm(y ~ -1 + meds, family = "poisson")
exp(glm1$coef * 8)


dat1 <- data.frame(y, xuse)
p1 <-  paste0("x", seq(1, ncol(dat1) - 1))
colnames(dat1)[-1] <- p1
eqn1 <- paste("y ~ ", paste(p1, collapse = " + "))
glm1 <- glm(eval(eqn1), data = dat1, family = "poisson")
gc1 <- glm1$coef[-1]
plot(ag1, gc1, ylim = c(-0.5, .5))
points(ag1, betas1, col = "red")
sglm1 <- summary(glm1)$coef[-1, ]
lb <- sglm1[, 1] - 1.96 * sglm1[, 2]

ub <- sglm1[, 1] + 1.96 * sglm1[, 2]
segments(x0 = ag1, y0 = lb, y1 = ub, col = 1)
```
